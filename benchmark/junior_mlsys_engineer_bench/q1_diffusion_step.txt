Research Question: How does reducing the number of sampling steps affect the inference time of a pre-trained diffusion model on MNIST? What is the relationship between them (linear or sub-linear)?
Model Configuration:
Diffusion Model: Pre-trained DDPM with U-Net backbone from Hugging Face (1000 timesteps).
U-Net: 3 down-blocks (64, 128, 256 filters), 3 up-blocks, time embedding dim=64.
Sampling Method: DDIM (deterministic sampling).
Noise Schedule: Linear beta schedule (beta_start=0.0001, beta_end=0.02).
Hyperparameters:
Sampling Steps: 10, 50, 100.
Batch Size: 1 (single image generation for precise timing).
Image Size: 28x28, grayscale.
Dataset:
MNIST: Generate 10 images from random noise.
Noise: Sampled from N(0, 1), shape (1, 1, 28, 28).
Evaluation Metrics:
Time per image generation (seconds, averaged over 10 images).
Additional Details:
Hardware: NVIDIA A40.
Framework: PyTorch with pre-trained weights (e.g., from Hugging Face).
Checkpoint: Assume publicly available MNIST DDPM model.
Timing: Measure full sampling process, including noise initialization.
