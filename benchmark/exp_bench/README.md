# EXP-Bench

EXP-Bench is our in-house, novel benchmark designed by the Curie team, to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. This dataset curates 460 AI research tasks from 51 top-tier AI research papers.

Link to the dataset: [HuggingFace](https://huggingface.co/datasets/Just-Curieous/EXP-Bench)