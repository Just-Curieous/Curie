Initial starter file can be found under '/starter_file/large_language_monkeys', which will be changed.
Here are more information:
 
1. You can setup the python environment for llmonk following the README.md. OpenAI Azure credentials are available under env.sh.

2. You can use the generator for gsm8k to generate a few response per question by running `python llmonk/generate/gsm8k.py` using model gpt-4o-mini. Make sure to always provide a new "save-dir" whenever this program is called.
 
3. You can evaluate the result of generated response under the saved dir from the  generator by running `python llmonk/evaluate/math_datasets.py`. Make sure to always provide a new "save_dir" whenever this program is called, and this cannot be the same --save-dir as was used in step 2.

4. Then, analyze the evaluation results yaml files for each problem under the "save_dir" in step 3. Look for a line formatted as "success: X" in the file, where X is the metric (a single boolean value) you need to retrieve, and report the success. We define success as: at least one generated sample solve the problem. 

Here is your question:

One way to scale language model inference compute is to repeatedly sample candidate solutions from a model. 
How does the number of generated samples (--num_samples) per question impact the overall success? 