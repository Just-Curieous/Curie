Can the accuracy impact of different prompting methods like Zero-shot-CoT and Auto-CoT be systematically improved by varying the number of reasoning steps without adding new content in a tightly controlled experiment setting, by using the following method: adding sentences that restate the question to increase reasoning steps?

Additional details:
- Test this for the last_letters dataset.
- Use GPT-4o-mini as the model.
- For Zero-shot-CoT: test using these 2 prompts, "Letâ€™s think step by step. You must think more steps.", and "Let's think step by step.".
- Feel free to refer to the code here: https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models