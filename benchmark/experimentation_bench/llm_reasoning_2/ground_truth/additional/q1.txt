# Design:

{
    "constant_vars": [
        "method for increasing reasoning_steps=Auto-CoT",
        "model=gpt-4o-mini"
    ],
    "independent_vars": [
        "reasoning_steps=at least 3 different positive reasoning step integer values, for each dataset",
        "datasets"="gsm8k, last_letters"
    ],
    "dependent_vars": [
        "accuracy"
    ],
}

# Setup:

Set up your Python environment as described in the repository documentation at: https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models

Inference Procedure

Use run_inference.py with args.method set to auto_cot and args.model set to gpt-4o-mini.

Run experiments on two datasets: gsm8k and last_letters.

Systematically vary the number of reasoning steps by specifying different demo prompt files provided (e.g., gsm8k_1, gsm8k_2, last_letters_1, last_letters_3, etc.).

Clearly organize and save outputs/logs for each reasoning-step configuration.

Evaluating Accuracy

Obtain accuracy results directly from the output logs generated by run_inference.py. Accuracy appears at the end of each log file and indicates the proportion of correctly solved problems for that run.

Analyzing Results

Compare the accuracy across different reasoning-step conditions for each dataset.

Determine whether increasing reasoning steps improves accuracy and identify if a saturation point emerges.