"The optimal number of reasoning steps vary across different LLMs. Using auto_cot, 
for gsm8k. For larger model, adding more steps seems to have a higher upperbound for accuracy.

for instance: 
Gpt-4o-mini: add 3 steps
Gpt-4o:add 3 steps
for Last_letters
Gpt-4o-mini: add 3 steps
Gpt-4o: add 4 steps"
