# Answer:

The optimal number of reasoning steps vary across different LLMs. For larger model, adding more steps seems to have a higher upperbound for accuracy.

For Last_letters:
Gpt-4o-mini: add 3 steps
Gpt-4o: add 4 steps