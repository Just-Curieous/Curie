How does the position of an incorrect step within the reasoning chain affect the overall outcome? Is an early error more detrimental than a later one?

Additional details:
- Use GPT-4o-mini as the model.
- Use the gsm8k dataset.
- Use the Auto-CoT method for increasing number of reasoning steps.
- The early error demo file is located in `gsm8k_early`, and the late demo file is in `gsm8k_later`, both in the repo below.
- Feel free to refer to the code here: https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models