Will increasing the number of reasoning steps in a Chain of Thought (CoT) prompt lead to higher accuracy in gpt-4o-mini up to a saturation point? 

Additional details:
- Test this for the gsm8k and last_letters datasets.
- Use the Auto-CoT method for increasing number of reasoning steps.
- Feel free to refer to the code here: https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models