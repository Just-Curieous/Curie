Question:
Considering that larger models generally perform better, would it be more cost-effective to use a smaller model with longer reasoning chains or a larger model with fewer steps for a given level of accuracy?

The code you need is available in /starter_file/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models

Instructions:
1. Set OpenAI credentials:
```
source /exp_agent/setup/env.sh
```
2. Activate conda environment:
```
conda activate impact
```
3. You will call `run_inference.py` with the following parameters. Make sure to read `run_inference.py` via cat first so you understand its contents.
    - Set the args.method to auto_cot
    - Set the args.model to gpt-4o-mini
    - Set the args.dataset to test 1 dataset: gsm8k 
```
python run_inference.py --dataset gsm8k --demo_path demo/gsm8k_3 --output_dir experiment/gpt-4o-mini/gsm8k_3  > log/gpt-4o-mini/gsm8k_3.log
```
- Execute these within the directory `/starter_file/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models`. This will ensure you don't face file path errors when running the commands above.
- Here are the demos available (you don't need to test all of them, just what makes sense): 
gsm8k_2
gsm8k_3
gsm8k_1

4. Test with **gpt-4o-mini**:
   You will call `run_inference.py` with the following parameters. Make sure to read `run_inference.py` via cat first so you understand its contents.
   - Set the following parameters:
     - `args.method`: `auto_cot`
     - `args.model`: `gpt-4o-mini`

5. Call run_inference.py with increasing number of reasoning steps. You can change the reasoning steps using different demo. In the above examples, last_letters_3 refers to add 3 reasoning steps, while last_letters_1 would refer to add 1 reasoning step. 
Optional: You can increase the args.max_length_cot in case the output of the model is truncated, and you need to view all of it. This may be useful when increasing the steps of cot.

6. Test with **gpt-4o**:
   You will call `run_inference.py` with the following parameters. Make sure to read `run_inference.py` via cat first so you understand its contents.
   - Set the following parameters:
     - `args.method`: `auto_cot`
     - `args.model`: `gpt-4o`

7. Call run_inference.py with increasing number of reasoning steps. You can change the reasoning steps using different demo. In the above examples, last_letters_3 refers to add 3 reasoning steps, while last_letters_1 would refer to add 1 reasoning step. 
Optional: You can increase the args.max_length_cot in case the output of the model is truncated, and you need to view all of it. This may be useful when increasing the steps of cot.

8. Compare costs for similar accuracy:
   - Find two log files—one from `gpt-4o-mini` and one from `gpt-4o`—with similar accuracy.
   - Use the `cost.py` file to compute the computational cost for each case:
    - Modify the input and output parameters in `cost.py` to reflect the models and log files you are comparing.
    - Choose the total_cost_4o_mini as the actual cost if you were using gpt-4o-mini
   - Run `cost.py` to compute the cost for each model and reasoning chain configuration.

9. Analyze and report:
   - Summarize your findings, including:
     - Dataset name.
     - Accuracy for both models.
     - Reasoning steps for each model.
     - Computational cost for achieving similar accuracy.
   - Discuss whether it is more cost-effective to use a smaller model (e.g., `gpt-4o-mini`) with longer reasoning chains or a larger model (e.g., `gpt-4o`) with fewer steps.
