Does the optimal number of reasoning steps vary across different LLMs (i.e., gpt-4o-mini and gpt-4o), and if so, what is the nature of this relationship? Your task is to determine the optimal reasoning step count for each model using the `last_letters` dataset.

Additional details:
- Use the Auto-CoT method for increasing number of reasoning steps.
- Feel free to refer to the code here: https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models