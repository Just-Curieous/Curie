The starter file can be found under "large_language_monkeys".
 
Here are more information:
 
1. You can setup the python environment for llmonk following the /starter_file/large_language_monkeys/README.md. Clean up the files under ./logs. OpenAI Azure credentials are available under /starter_file/large_language_monkeys/env.sh

2. You can use the generator for gsm8k to generate a few response per question by running `python llmonk/generate/gsm8k.py` using model gpt-4o-mini. Make sure to always provide a new "save-dir" whenever this program is called.
 
3. You can evaluate the result of generated response under the saved dir from the  generator by running `python llmonk/evaluate/math_datasets.py`. Make sure to always provide a new "save_dir" whenever this program is called, and this cannot be the same --save-dir as was used in step 2.
 
4. Then, analyze the evaluation results files (these are yaml files) for each problem under the "save_dir" in step 3. Look for a line formatted as "success: X" in the file, where X is the metric (a single boolean value) you need to retrieve, and report the success. Here is how success is defined: it is true if the problem is successfully solved by at least one generated sample, otherwise it is false. There will be a line containing success, if you don't see that at all, you have made errors in the previous steps. 

Note: there is no need to modify the source code of any files within /starter_file. Though you may choose to read the source code just to enhance your understanding of its inner workings.

Note: it is alright if the success is false for most of the problems. Do not question the integrity of the source code of any files within /starter_file.

Here is your question:

One way to scale language model inference compute is to repeatedly sample candidate solutions from a model. 
Considering that a larger, more capable model (e.g., gpt-4o) costs significantly more per query compared to a smaller model (e.g., gpt-4o-mini), would it be feasible to use the smaller model, sample more responses, and achieve comparable rate of success while being more cost-effective? 
Note that GPT-4o is 16 times more expensive than gpt-4o-mini for both prompt and response tokens.