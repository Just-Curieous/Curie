Considering that a larger, more capable model (e.g., gpt-4o) costs significantly more per query compared to a smaller model (e.g., gpt-4o-mini), would it be feasible to use the smaller model, sample more responses, and achieve comparable rate of success while being more cost-effective? 
Additional details:
- Note that GPT-4o is 16 times more expensive than gpt-4o-mini for both prompt and response tokens.
- The success rate is defined as the proportion of questions correctly answered by at least one generated sample. 
- Use the GSM8K math dataset.