The starter file can be found under "large_language_monkeys".
 
Here are more information:
 
1. You can setup the python environment for llmonk following the /starter_file/large_language_monkeys/README.md. Clean up the files under ./logs. OpenAI Azure credentials are available under /starter_file/large_language_monkeys/env.sh

2. You can use the generator for gsm8k to generate a few response per question by running `python llmonk/generate/gsm8k.py` using model gpt-4o-mini. Make sure to always provide a new "save-dir" whenever this program is called.
 
3. You can evaluate the result of generated response under the saved dir from the  generator by running `python llmonk/evaluate/math_datasets.py`. Make sure to always provide a new "save_dir" whenever this program is called, and this cannot be the same --save-dir as was used in step 2.
  
4. You can get the final response by aggregating the evaluation results for each problem using majority vote, 
The voter counts the occurrences of each unique answer within the input set. It identifies the answer that appears most frequently.
You can determine the accuracy by comparing the ground truth answer and the aggregated response. 

Note: there is no need to modify the source code of any files within /starter_file. Though you may choose to read the source code just to enhance your understanding of its inner workings.

Note: it is alright if the accuracy is low for most of the problems. Do not question the integrity of the source code of any files within /starter_file.

Here is your question: 
One approach to scaling language model inference is to repeatedly sample candidate solutions from the model and aggregate them using majority voting. 
How does the number of samples impact the overall accuracy on the GSM8K task?