One way to scale language model inference compute is to repeatedly sample candidate solutions from a model. 
How does the number of generated samples (--num_samples) per question impact the overall success? 