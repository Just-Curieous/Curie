Task description:
Feedback is a text analysis task to predict or classify user feedback, often involving sentiment or relevance analysis.

Here is your task:
- The starter file can be found under `MLAgentBench/benchmarks/feedback`.
- Go through the env/data_description.txt file to understand the data and the machine learning task. You can summarize it in your research logs to keep track of what all you have to do.
- Then use the provided train.py script to train a language model on CPU to get the best performance (minimize MCRMSE). You MUST "cat" to read the script and then modify it. You must execute it by going into the env/ directory:
```

# -------------------------------------
# Docker Setup (Recommended)
# -------------------------------------

# 1. Pull the pre-built Docker image:
docker pull qhwang123/researchassistant:latest

# 2. Run the container with necessary mounts:
# Without GPU:
docker run -it --user root -v "$(pwd)":/MLAgentBench -w /MLAgentBench qhwang123/researchassistant:latest

# With GPU:
docker run -it --user root --gpus all -v "$(pwd)":/MLAgentBench -w /MLAgentBench qhwang123/researchassistant:latest

# 3. Install dependencies inside the container:
apt update && apt install -y wget unzip

# 4. Configure Kaggle API Key:
# Install Kaggle CLI and configure API key inside the Docker container
pip install kaggle

# Ensure the Kaggle config directory exists
mkdir -p ~/.kaggle

# Copy the Kaggle API key from the mounted host directory
cp /starter_file/kaggle.json ~/.kaggle/kaggle.json

# Set the correct permissions for security
chmod 600 ~/.kaggle/kaggle.json

# 5. Prepare the feedback task:
python -u -m MLAgentBench.prepare_task feedback

# 6. Train the model:
cd starter_file/MLAgentBench/MLAgentBench/benchmarks/feedback/env
python train.py

Goal:
Start by running and analyzing the baseline model. Then iteratively modify the model code and configuration to improve performance. For each modification:

Explain the change and its motivation
Update the code accordingly
Retrain the model
Record the new MCRMSE and per-category RMSE
Compare with previous runs to determine whether performance improved
All experiments and outcomes should be logged clearly for final reporting.