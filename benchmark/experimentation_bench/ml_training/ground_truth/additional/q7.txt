# Design:

{
    "constant_vars": [
        "dataset=BabyLM child-directed linguistic dataset",
        "implementation_framework=Huggingface Transformers library"
    ],
    "independent_vars": [
        "model_architecture=any valid language modeling architecture (e.g., GPT, Transformer variants)",
        "hyperparameters=any valid hyperparameters for the chosen architecture, e.g., learning rate, optimizer, batch size, epochs",
    ],
    "dependent_vars": [
        "validation perplexity"
    ]
}

# Setup: refer to instructions in question file.