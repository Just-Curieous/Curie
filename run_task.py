import curie
key_dict = {
    "MODEL": "gpt-3.5-turbo",
    "AZURE_API_VERSION": "2025-01-01-preview",
    "AZURE_API_KEY": "bdef9bf150194da5b8ad860046aff989",
    "ORGANIZATION": "327403",
    "AZURE_API_BASE": "https://api.umgpt.umich.edu/azure-openai-api"
}

result = curie.experiment(
    api_keys=key_dict,
    question="You are a highly capable researcher. Your job is to solve a given scientific experiment task based on a real paper, which will require you to formulate hypothesis, design an experiment, write and execute experiment code, analyzing results, and produce a conclusion. You are not allowed to read the paper itself (e.g., the PDF of the research paper). You are not allowed to perform any Git operations, such as checking out commits, switching branches, or accessing other versions of the repository. Work only with the current files and contents as given. Do not use the README to obtain results that the question expects you to derive through experimentation. Only reference the README for information not required to be experimentally obtained, such as baseline results or general setup details. Don't just write code, but also execute code, analyze and produce conclusions. Operate strictly within the provided code repo. Save any written code as a file in the repo. \n\nThe task will be provided as input to you in the form of: a question, description of the method, and optionally specific instructions (labelled as \"agent_instructions\").\n\nOutput your response in the following format in valid JSON:\n{\n  \"design\": \"string or [list, of, strings]\",\n  \"conclusion\": \"...\"\n}\n\nExplanation of output keys:\n- \"design\": Describe your experiment design. This could include experiment variables (i.e., independent, dependent, and control variables), or a general outline of the experimental method. Try your best to use the former. Also, design can be specified as a single string or a list of design steps.\n- \"conclusion\": State your final conclusion based on the experiment you conducted, grounded in the results from your code execution. Provide a general relationship or concrete metrics that answers the research question (e.g., numerical improvement, performance gap, statistical significance, etc.).\n\nInput: \n\nQuestion: Does the MDGEN model generate transition paths that are statistically more consistent with the reference MSM than transition paths sampled from replicate MD-derived MSMs of shorter durations?\n\nMethod: #### Problem Setup\n\nEvaluate MDGEN’s ability to zero-shot sample accurate transition trajectories between metastable states in tetrapeptides, comparing against MSMs built from shorter MD simulations.\n\n#### Independent Variables\n\n- **Trajectory Source**:\n  - MDGEN model (1 ns trajectory conditioned on first and last frame)\n  - MSMs built from replicate MD simulations of various durations: 50 ns, 30 ns, 20 ns, 10 ns\n\n#### Dependent Variables (Evaluation Metrics)\n\n1. **Path Likelihood** under the reference MSM (average transition probability between start/end states)\n2. **Fraction of Valid Paths** (non-zero probability under the reference MSM)\n3. **Jensen-Shannon Divergence (JSD)** between visited state distributions and the reference MSM\n\n#### Experiment Components and Setup\n\n- **Reference MSM**: Constructed from a 100 ns MD trajectory and defines 10 metastable states. This is used for state discretization, computing flux and path likelihood\n\n- **Trajectory Generation**:\n\n  1. For each of 100 test peptides, identify the two most well-separated metastable states (lowest flux pair).\n  2. Using MDGEN, generate 1 ns transition trajectories (100 frames), conditioned on the initial and final frames.\n  3. Generate an ensemble of 1000 paths per peptide.\n\n- **Trajectory Discretization**:\n\n  - Discretize each generated path into the 10 reference MSM states\n\n- **Evaluation Metrics**:\n\n  - **Path Likelihood** under the reference MSM\n  - **Fraction of Valid Paths** (paths with non-zero start/end visitation probability)\n  - **JSD** between categorical state visitation histograms of MDGEN vs. reference\n\n- **Baseline Comparison**:\n\n  - Repeat full pipeline using MSMs trained on replicate MD trajectories with varying durations (10–50 ns)\n\n  - Metrics averaged over all 100 test peptides and 1000 transition paths per peptide\n  - Reference Figure 3 for visual analysis, Tables 2 and 4 for runtime comparisons\n\nAgent Instructions: Your task is to implement a system for generating and analyzing transition paths in molecular dynamics simulations. The system consists of two main components:\n\n1. A transition path generator that uses a pre-trained MDGEN model to create paths between conformational states of peptides.\n2. An analysis tool that evaluates the quality of these generated paths compared to reference molecular dynamics simulations.\n\nFor the transition path generator:\n- Load a pre-trained MDGEN model from a checkpoint\n- Process peptide data from specified directories\n- For each peptide, build a Markov State Model (MSM) from reference MD trajectories\n- Identify meaningful start and end states by analyzing the flux between metastable states\n- Generate transition paths between these states using the MDGEN model\n- Save the generated paths in appropriate molecular formats (PDB, XTC) with metadata\n\nFor the analysis tool:\n- Load the generated transition paths and reference MSM data\n- Calculate three key metrics to evaluate path quality:\n  a) Path Likelihood under the reference MSM\n  b) Fraction of Valid Paths\n  c) Jensen-Shannon Divergence (JSD) between visited state distributions\n- Compare the generated paths against MSMs built from replicate MD simulations of various durations (50ns, 30ns, 20ns, 10ns, 5ns, 2ns)\n- Visualize results with plots showing state distributions and transition paths\n- Save analysis results for further processing\n\nThe implementation should use PyEMMA for Markov modeling, MDTraj for trajectory manipulation, and appropriate visualization libraries for plotting results.\n\nLLM related credentials (if needed) are available in /workspace/setup_apis_exp/\n\nPlease save your response JSON to: outputs/tmp_prompt_gen/93022_task_index_1_iter_1_duration_0.67_eval_gen.json\n\nIn addition, you must create a single shell script named `reproduce_exp_bench.sh`. This script must reproduce the entire experiment from start to finish, including:\n- Any necessary environment setup (e.g., installing dependencies)\n- Running the experiment (e.g., other scripts)\n- Producing the output or result used in your conclusion\n\nWe will use this script to verify your experiment's reproducibility. Make sure it can be run from the root of the repo and reproduces your result end-to-end.",
    task_config={
        "job_name": "default_research",
        "docker_image": "amberljc/curie:latest",
        "dockerfile_name": "ExpDockerfile_pip",
        "benchmark_specific_context": "none",
        "is_user_interrupt_allowed": False,
        "timeout": 600,
        "max_coding_iterations": 10,
        "max_global_steps": 10,
        "supervisor_system_prompt_filename": "prompts/simple/simple-supervisor.txt",
        "control_worker_system_prompt_filename": "prompts/simple/simple-control-worker.txt",
        "patcher_system_prompt_filename": "prompts/simple/simple-patcher.txt",
        "llm_verifier_system_prompt_filename": "prompts/simple/simple-llm-verifier.txt",
        "coding_prompt_filename": "prompts/simple/simple-coding.txt",
        "worker_system_prompt_filename": "prompts/simple/simple-worker.txt",
        "workspace_name": "workspace/mdgen_20250801_163406_task_index_7_iter_1_duration_0.67",
        "dataset_dir": "",
        "env_requirements": "",
        "code_instructions": ""
    }
)
